{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# auto differentiation library\n",
    "import torch\n",
    "from torch.autograd import grad, Function\n",
    "import torch.nn.functional as F  # usual functions\n",
    "import torch.nn as nn  # for defining Neural Networks\n",
    "import torch.optim as optim\n",
    "\n",
    "# notebook specific library for displays\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch's Tensors\n",
    "\n",
    "Pytorch contains a class `Tensor` which is a wrapper for array, matrices, tensors... But numpy is already able to handle usual linear algebra operation and coordinate-wise operations. Pytorch brings to the table __auto-differentiation__ and __GPU operations__.\n",
    "That means a Tensor contains everything necessary to backpropagate gradients through the operation which created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a Tensor from a numpy array\n",
    "x = np.array([3, 1, 4, 1, 5, 9, 2])\n",
    "x = torch.Tensor(x)\n",
    "print(\"Tensor type `x.type()` gives more insight on the data structure of tensor coordinates: {}\".format(x.type()))\n",
    "print(\"The tensor's shape, alternatively `x.size()` or `x.shape`: {}\".format(x.shape))\n",
    "print(x)\n",
    "print(\"You can get a numpy array back with `x.numpy()`: {}\".format(x.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra operation\n",
    "A = np.random.randn(3, x.shape[0])\n",
    "A = torch.Tensor(A)\n",
    "\n",
    "print(\"`torch.mv` for matrix-vector multiplication: {}\".format(torch.mv(A, x)))\n",
    "print(\"Multiplication `*` is reserved for coordinate wise multiplication.\")\n",
    "print(\"In Python, matrix multiplication is done using `@`: {}\".format(A @ x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch's Gradient Computation\n",
    "\n",
    "Let's say $y = f(x) \\in \\mathbb R$ where $f$ is a known function and $x$ is a tensor. We know how to compute the gradient of $f$ at point $x$: for Pytorch, \"knowing\" a function means knowing both how to compute its value at a point $x \\mapsto f(x)$, but also knowing how to compute its gradient $x \\mapsto \\nabla f(x)$.\n",
    "\n",
    "When computing $y$, Pytorch records that it was obtained from calling the function $f$ at point $x$. Here is an example of this computation using only one variable $y = \\| x \\|_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, requires_grad=True)\n",
    "y = torch.norm(x, p='fro')\n",
    "display(Markdown(r\"Pytorch knows how y was defined, it records how to compute compute gradients of y: $\\nabla \\|\\,.\\|_2$ is stored as '{}'\".format(y.grad_fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing a gradient with Pytorch\n",
    "\n",
    "In this context, $y$ represents an error (or \"loss\" as it is often called in machine learning) that we want to minimize with a gradient descent. \n",
    "\n",
    "If the user wants to compute the gradient of $y$ with respect to $x$:\n",
    "- The gradient we want to compute is the gradient of $y$ with respect to other variables: we know that $y = f(x)$ so we want to compute $\\nabla f(x)$.\n",
    "- $f$ is a \"known\" function, so pytorch has access to $\\nabla f$, and it has recorded the value of $x$ as well: it simply needs to compute $\\nabla f(x)$.\n",
    "\n",
    "This computation is done when calling `torch.grad` which was imported as `grad`. It needs two arguments:\n",
    "- The loss whose gradient we want to compute $y$\n",
    "- The list of variables with respect to which we want to compute the gradient: if $\\mathcal y = f(x, z)$, and we want the gradient with respect to both $x = (x_1, \\dots, x_n)$ and $z = (z_1, \\dots, z_m)$, then the output will be\n",
    "$$\n",
    "    \\nabla f(x, z) = \\left(\n",
    "        \\frac{\\partial \\mathcal L}{\\partial x_1}, \\dots, \\frac{\\partial \\mathcal L}{\\partial x_n},\n",
    "        \\frac{\\partial \\mathcal L}{\\partial z_1}, \\dots, \\frac{\\partial \\mathcal L}{\\partial z_m}\n",
    "    \\right) = \\left( \\nabla_x f(x, z) \\ | \\ \\nabla_z f(x, z) \\right)\n",
    "$$\n",
    "\n",
    "Here is what it looks like with the previous example $y = \\| x \\|_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_y = grad(y, [x])\n",
    "print(\"grad returns a list of gradients refering to the list of arguments [x]: {}\".format(grad_y))\n",
    "grad_y_x = grad_y[0]  # index 0 because x is at index 0 in the list [x]\n",
    "display(grad_y_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 - Sanity check: compute the gradient $\\nabla_x y$ by hand and compare your result to `grad_y_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other differential operators:\n",
    "\n",
    "Know that the `torch.autograd` package contains automatic computations for other differential operators. For example, the function `jacobian` extends `grad` to non-scalar inputs and the function `hessian` computes second order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First mention of a neural network\n",
    "\n",
    "Let's present an abstract description of neural networks through a basic example: recognizing photos of cats and dogs. \n",
    "- $x \\in \\mathbb R^d$ contains a data sample: in this example $x$ would be the image, but in other contexts it could be sounds, medical records, GPS informations, internet history, DNA information, etc... or even a combination of multiple of those.\n",
    "- For a binary classification task, $y \\in \\{0, 1\\}$ represents the prediction that we make for that sample e.g. if the image in $x$ is more likely to be a cat $0$ or a dog $1$ ?\n",
    "\n",
    "Often, we will assume that there exists an _oracle_ function $f:\\, \\mathbb R^d \\mapsto \\{0, 1\\}$ such that $f(x)$ is always the correct answer, or at least the best$^*$ answer that can be given.\n",
    "\n",
    "*: here, the \"best\" is defined in relation to an error that must be defined as part of the mathematical formulation of the problem we are confronted to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An abstract neural network\n",
    "\n",
    "In general, neural networks are a class of parametrized functions $f_\\theta$ mapping an input to a prediction: $y = f_\\theta(x)$. Here, $\\theta \\in \\mathbb R^p$ is the set of parameters used by the network.\n",
    "\n",
    "The goal is to make good prediction, _i.e._ to estimate the class of any probable data sample $x$: $\\tilde y = f_\\theta(x)$ approximates $y = f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning technical jargon\n",
    "In supervized learning, we fix the parameters $\\theta$ with what refer to as training, using a dataset.\n",
    "- A dataset is a set of pairings between data samples $x^{(1)} \\in \\mathbb R^d,\\, \\dots,\\, x^{n} \\in \\mathbb R^d$, and the corresponding correct answer:\n",
    "    - for example, humans are very good at recognizing cats from dogs on images so the network can be trained to match human performance\n",
    "    - in other contexts, the correct answer can be an objective truth: did the meteorological $x^{(i)}$ result in rain one hour later ? did the patient carrying DNA $x^{(i)}$ develop a specific cancer before the age of 50 ? Did the internet user with the internet history $x^{(i)}$ buy the product advertised ?\n",
    "- We can rewrite $f_\\theta(x) = f(x, \\theta)$. $f$ is a function defined by the user, so its gradient is known: we don't care much about $\\nabla_x f$, but $nabla_\\theta f$ allows us to optimise $\\theta$ through a gradient decent. Training is just technical jargon for this gradient descent.\n",
    "\n",
    "Note: to have well defined gradients, we can't use $\\tilde y = f_\\theta(x) \\in \\{0, 1\\}$ and relax it to $\\tilde y \\in [0, 1]$ or even $\\tilde y \\in \\mathbb R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing $\\nabla_\\theta f$\n",
    "In general, $f_\\theta$ can be very complicated and include several milions of parameters: deriving its gradient by hand can be tidious if not physically impossible.\n",
    "\n",
    "However, $f_\\theta$ is often defined as a cascade of simple functions: linear functions $A_k: \\mathbb R^{d_k} \\mapsto \\mathbb R^{d_{k+1}}$ and non-linear activations $\\rho_k: \\mathbb R \\mapsto \\mathbb R$ applied to each coordinate independantly. The parameters here are the matrices $A_k$ which are regrouped into $\\theta = \\left( A_1 | \\dots | A_K \\right)$.\n",
    "\n",
    "This structure allows us to compute all gradients\n",
    "$$\n",
    "    \\nabla_\\theta f(x, \\theta) = \\left(\\ \\nabla_{A_1} f(x, \\theta) \\,| \\dots \\,|\\, \\nabla_{A_K} f(x, \\theta) \\ \\right)\n",
    "$$\n",
    "using the chain-rule. The resulting algorithm is called backpropagation, we will study it next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent in Pytorch\n",
    "\n",
    "Let $A \\in \\mathcal M_{p, n}(\\mathbb R)$ and $y \\in \\mathbb R^p$. We want to find the pseudo inverse $x = A^{<-1>}y \\in \\mathbb R^n$ of $y$. This is typically done by defining\n",
    "$$\n",
    "    \\tilde x = argmin_{x \\in \\mathbb R^n} \\| Ax - y \\|_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, n = 3, 4\n",
    "A = torch.randn(p, n)\n",
    "y = torch.randn(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Using a random initialization and Pytorch's SGD (stochastic gradient descent): https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.SGD , compute $\\tilde x$. Plot the evolution of $\\| Ax - y \\|_2^2$ during the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions in Pytorch\n",
    "\n",
    "Pytorch already knows many elementary functions such as sums, sine functions, norms, etc... Other functions can be obtained through the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Define the function $f(x, A, b) = \\max(0, Ax + b)$ where $x \\in \\mathbb R^{d_0}$, $A \\in \\mathcal M_{d_0, d_1}(\\mathbb R)$ and $b \\in \\mathbb R^{d_1}$.\n",
    "\n",
    "Note: $x \\mapsto \\max(0, x)$ is a widely used operation in deep learning, commonly refered to as ReLU for _Rectified Linear Unit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, A, b):\n",
    "    \"\"\"Compute the result of a fully connected layer y = max(0, Ax+b).\"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0, d1 = 4, 3\n",
    "\n",
    "x = torch.randn(d0, requires_grad=True)\n",
    "A = torch.randn(d1, d0, requires_grad=True)\n",
    "b = torch.randn(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Compute $\\|f(x, A, b)\\|_2$ and its gradient with respect to $A$ and $b$ using the function `grad` as before. How could Pytorch know how to compute those gradients ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining new functions\n",
    "\n",
    "If the function we want to use can not be defined using basic Pytorch functions (typically because one step is unstable or has unstable gradient computation), we must define it ourselves.\n",
    "\n",
    "Pytorch's autograd package provides the class `Function` for this exact purpose:\n",
    "    https://pytorch.org/docs/stable/autograd.html?highlight=function#torch.autograd.Function\n",
    "\n",
    "Recall that in Pytorch, \"knowing\" a function a function means knowing how to compute it, as well as knowing how to compute its gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5: Define the inverse matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseMatrix(Function):\n",
    "    \"\"\"A Function class for computing the inverse matrix, and its gradient.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, A):\n",
    "        \"\"\"Compute the inverse of matrix A\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"Compute the gradient of the loss with respect to A.\n",
    "        \n",
    "        grad_ouput contains the gradient of the loss with respect to A^-1\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
