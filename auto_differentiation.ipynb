{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# auto differentiation library\n",
    "import torch\n",
    "from torch.autograd import grad, Function\n",
    "import torch.nn.functional as F  # usual functions\n",
    "import torch.nn as nn  # for defining Neural Networks\n",
    "import torch.optim as optim\n",
    "\n",
    "# notebook specific library for displays\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch's Tensors\n",
    "\n",
    "Pytorch contains a class `Tensor` which is a wrapper for array, matrices, tensors... But numpy is already able to handle usual linear algebra operation and coordinate-wise operations. Pytorch brings to the table __auto-differentiation__ and __GPU operations__.\n",
    "That means a Tensor contains everything necessary to backpropagate gradients through the operation which created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a Tensor from a numpy array\n",
    "x = np.array([3, 1, 4, 1, 5, 9, 2])\n",
    "x = torch.Tensor(x)\n",
    "print(\"Tensor type `x.type()` gives more insight on the data structure of tensor coordinates: {}\".format(x.type()))\n",
    "print(\"The tensor's shape, alternatively `x.size()` or `x.shape`: {}\".format(x.shape))\n",
    "print(x)\n",
    "print(\"You can get a numpy array back with `x.numpy()`: {}\".format(x.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra operation\n",
    "A = np.random.randn(3, x.shape[0])\n",
    "A = torch.Tensor(A)\n",
    "\n",
    "print(\"`torch.mv` for matrix-vector multiplication: {}\".format(torch.mv(A, x)))\n",
    "print(\"Multiplication `*` is reserved for coordinate wise multiplication.\")\n",
    "print(\"In Python, matrix multiplication is done using `@`: {}\".format(A @ x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch's Gradient Computation\n",
    "\n",
    "Let's say $y = f(x) \\in \\mathbb R$ where $f$ is a known function and $x$ is a tensor. We know how to compute the gradient of $f$ at point $x$: for Pytorch, \"knowing\" a function means knowing both how to compute its value at a point $x \\mapsto f(x)$, but also knowing how to compute its gradient $x \\mapsto \\nabla f(x)$.\n",
    "\n",
    "When computing $y$, Pytorch records that it was obtained from calling the function $f$ at point $x$. Here is an example of this computation using only one variable $y = \\| x \\|_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, requires_grad=True)\n",
    "y = torch.norm(x, p='fro')\n",
    "display(Markdown(r\"Pytorch knows how y was defined, it records how to compute compute gradients of y: $\\nabla \\|\\,.\\|_2$ is stored as '{}'\".format(y.grad_fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing a gradient with Pytorch\n",
    "\n",
    "In this context, $y$ represents an error (or \"loss\" as it is often called in machine learning) that we want to minimize with a gradient descent. \n",
    "\n",
    "If the user wants to compute the gradient of $y$ with respect to $x$:\n",
    "- The gradient we want to compute is the gradient of $y$ with respect to other variables: we know that $y = f(x)$ so we want to compute $\\nabla f(x)$.\n",
    "- $f$ is a \"known\" function, so pytorch has access to $\\nabla f$, and it has recorded the value of $x$ as well: it simply needs to compute $\\nabla f(x)$.\n",
    "\n",
    "This computation is done when calling `torch.grad` which was imported as `grad`. It needs two arguments:\n",
    "- The loss whose gradient we want to compute $y$\n",
    "- The list of variables with respect to which we want to compute the gradient: if $\\mathcal y = f(x, z)$, and we want the gradient with respect to both $x = (x_1, \\dots, x_n)$ and $z = (z_1, \\dots, z_m)$, then the output will be\n",
    "$$\n",
    "    \\nabla f(x, z) = \\left(\n",
    "        \\frac{\\partial \\mathcal L}{\\partial x_1}, \\dots, \\frac{\\partial \\mathcal L}{\\partial x_n},\n",
    "        \\frac{\\partial \\mathcal L}{\\partial z_1}, \\dots, \\frac{\\partial \\mathcal L}{\\partial z_m}\n",
    "    \\right) = \\left( \\nabla_x f(x, z) \\ | \\ \\nabla_z f(x, z) \\right)\n",
    "$$\n",
    "\n",
    "Here is what it looks like with the previous example $y = \\| x \\|_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_y = grad(y, [x])\n",
    "print(\"grad returns a list of gradients refering to the list of arguments [x]: {}\".format(grad_y))\n",
    "grad_y_x = grad_y[0]  # index 0 because x is at index 0 in the list [x]\n",
    "display(grad_y_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 - Sanity check: compute the gradient $\\nabla_x y$ by hand and compare your result to `grad_y_x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run -i solutions/exo1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other differential operators:\n",
    "\n",
    "Know that the `torch.autograd` package contains automatic computations for other differential operators. For example, the function `jacobian` extends `grad` to non-scalar inputs and the function `hessian` computes second order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First mention of a neural network\n",
    "\n",
    "Let's present an abstract description of neural networks through a basic example: recognizing photos of cats and dogs. \n",
    "- $x \\in \\mathbb R^d$ contains a data sample: in this example $x$ would be the image, but in other contexts it could be sounds, medical records, GPS informations, internet history, DNA information, etc... or even a combination of multiple of those.\n",
    "- For a binary classification task, $y \\in \\{0, 1\\}$ represents the prediction that we make for that sample e.g. if the image in $x$ is more likely to be a cat $0$ or a dog $1$ ?\n",
    "\n",
    "Often, we will assume that there exists an _oracle_ function $f:\\, \\mathbb R^d \\mapsto \\{0, 1\\}$ such that $f(x)$ is always the correct answer, or at least the best$^*$ answer that can be given.\n",
    "\n",
    "*: here, the \"best\" is defined in relation to an error that must be defined as part of the mathematical formulation of the problem we are confronted to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An abstract neural network\n",
    "\n",
    "In general, neural networks are a class of parametrized functions $f_\\theta$ mapping an input to a prediction: $y = f_\\theta(x)$. Here, $\\theta \\in \\mathbb R^p$ is the set of parameters used by the network.\n",
    "\n",
    "The goal is to make good prediction, _i.e._ to estimate the class of any probable data sample $x$: $\\tilde y = f_\\theta(x)$ approximates $y = f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent in Pytorch\n",
    "\n",
    "Let $A \\in \\mathcal M_{q, p}(\\mathbb R)$ and $y \\in \\mathbb R^q$. We want to find the pseudo inverse $x = A^{<-1>}y \\in \\mathbb R^p$ of $y$. This is typically done by defining\n",
    "$$\n",
    "    \\tilde x = argmin_{x \\in \\mathbb R^p} \\| Ax - y \\|_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, p = 3, 4\n",
    "A = torch.randn(q, p)\n",
    "y = torch.randn(q)\n",
    "\n",
    "lr = 1e-2\n",
    "niter = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Using a random initialization and Pytorch's `grad` function, implement by hand a gradient descent to minimize $\\| Ax - y \\|_2^2$ with learning rate (step size) `lr` and `niter` steps. Plot the evolution of $\\| Ax - y \\|_2^2$ during the descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run -i solutions/exo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute a gradient, you can also call `loss.backward` on a scalar `loss`: the gradient $\\nabla_x loss$ will be computed for all $x$ on which `loss` depends, and stored in `x.grad`.\n",
    "\n",
    "Pytorch provides `Optimizer`s to seamlessly perform gradient descents, one example is SGD (stochastic gradient descent): https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.SGD.\n",
    "An optimizer encapsulates the whole gradient step scheme: it knows which variables must be updated, and how to update them given the corresponding gradient.\n",
    "- The convention is that the list of variables to update is given along with all descent parameters (e.g. step size) when defining the optimizer.\n",
    "- As for the gradients, the optimizer will conveniently look for the gradient corresponding to `x` in `x.grad`, making it perfect to use alongside the `backward` method.\n",
    "- However, since the gradient is stored in `x.grad`, we will have to reset it between each iteration. This is also automated by the optimizer using `optimizer.zero_grad()` (sets gradient with respect to all optimized variables to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(p, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([x], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Same as Exercise 2 using the optimizer defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run -i solutions/exo3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning technical jargon\n",
    "In supervized learning, we fix the parameters $\\theta$ with what refer to as training, using a dataset.\n",
    "- A __dataset__ is a set of pairings between data samples $x^{(1)} \\in \\mathbb R^d,\\, \\dots,\\, x^{n} \\in \\mathbb R^d$, and the corresponding correct answer:\n",
    "    - for example, humans are very good at recognizing cats from dogs on images so the network can be trained to match human performance\n",
    "    - in other contexts, the correct answer can be an objective truth: did the meteorological $x^{(i)}$ result in rain one hour later ? did the patient carrying DNA $x^{(i)}$ develop a specific cancer before the age of 50 ? Did the internet user with the internet history $x^{(i)}$ buy the product advertised ?\n",
    "    - we call this _correct answer_ the sample's __label__.\n",
    "- We can rewrite $f_\\theta(x) = f(x, \\theta)$. $f$ is a function defined by the user, so its gradient is known: we don't care much about $\\nabla_x f$, but $nabla_\\theta f$ allows us to optimise $\\theta$ through a gradient decent. __Training__ is just technical jargon for this gradient descent.\n",
    "\n",
    "Note: to have well defined gradients, we can't use $\\tilde y = f_\\theta(x) \\in \\{0, 1\\}$ and relax it to $\\tilde y \\in [0, 1]$ or even $\\tilde y \\in \\mathbb R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "We define a fully random dataset: points $x \\sim \\mathcal N(0, 1)$ and associated labels $y \\sim Bernoulli(1/2)$. Of course this is very simplistic, but it will allow us to define our first models and to point out some classic neural network caveats.\n",
    "\n",
    "Pytorch provides abstract representation of dataset with the `Dataset`, from which multiple classes inherit depending on the nature of the dataset. Here we will use `torch.utils.data.TensorDataset`since both `x` and `y` are already tensors.\n",
    "\n",
    "A `DataLoader` class is also provided to be able to seamlessly iterate over all samples in the dataset in small batches (subset small enough to be able to fit in memory but big enough for the stochastic gradient descent to be efficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000  # number of training samples\n",
    "p = 20  # dimension of each sample\n",
    "x = torch.randn(n, p)  # each of the n samples is in R^p\n",
    "y = torch.randint(0, 1, (n,), dtype=x.dtype)  # each of the n labels is in {0, 1}\n",
    "print(\"x:\", x.shape)\n",
    "print(\"y:\", y.shape)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"\\nWe can now iterate over dataloader to get a subset of x with the corresponding subset of y:\")\n",
    "for x_batch, y_batch in dataloader:\n",
    "    print(\"    batch of samples:\", x_batch.shape, \"    corresponding labels:\", y_batch.shape)\n",
    "print(\"Notice that the last batch is incomplete as 1000 is not a multiple of 32.\\n\"\n",
    "      \"Pytorch takes care of it so we don't have to worry about it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "We start with a logistic regression:\n",
    "$$\n",
    "    \\tilde y = f_\\theta(x) = \\sigma(Ax + b)\n",
    "$$\n",
    "where $f_\\theta$ is the model, $\\theta = (A, b)$ represent all its parameters and $\\sigma: z\\in\\mathbb R \\mapsto \\frac{1}{1 + \\exp(z)}$ is the sigmoid function.\n",
    "Here, $\\tilde y \\in [0, 1]$ is supposed to approximate the probability $p(y=1|x)$.\n",
    "\n",
    "In pytorch, we define model (or more generally parameterized functions) as new classes which inherit from nn.Module. When doing so, it is implied that all functions used in the model are _known_ (able to compute the function's image and gradient) by Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \"\"\"A Logistic Regression Module implemented in Pytorch.\n",
    "    \n",
    "    Input:\n",
    "        p: int\n",
    "            Dimension of the input space\n",
    "    \n",
    "    Argument:\n",
    "        x: torch.Tensor\n",
    "            Input tensor of shape [B, p] where B is an arbitrary batch size.\n",
    "\n",
    "    Output:\n",
    "        y_tilde: torch.Tensor\n",
    "            Approximation of p(y=1|x) under a logistic regression model, with shape [B,].\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p):\n",
    "        \"\"\"Initialize a LogisticRegression model.\n",
    "        \n",
    "        notes:\n",
    "            - This method is called when calling 'model = LogisticRegression(p)'.\n",
    "            - The first argument self is not defined by the user, it is a way for the object to manipulate itself.\n",
    "        \"\"\"\n",
    "        super().__init__()  # call the __init__ method of nn.Module from which LogisticRegression inherits.\n",
    "        self.linear = nn.Linear(p, 1, bias=True)  # linear layer, contains the weights A and the bias b.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Return y_tilde = sigma(Ax + b).\n",
    "        \n",
    "        Because of nn.Module, this method is what will be called when writing model(x).\n",
    "        \"\"\"\n",
    "        y_tilde = torch.sigmoid(self.linear(x))\n",
    "        # remove dimension 1 which is now useless\n",
    "        y_tilde = y_tilde.squeeze(1)\n",
    "        return y_tilde\n",
    "\n",
    "model = LogisticRegression(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing the model's parameters with gradient descent\n",
    "\n",
    "We made our model inherit from `nn.Module`, which make a lot of tedious work automatic. For example, when defining the optimizer, we will need to know all the model's parameters: the list is given by `model.parameters()`.\n",
    "\n",
    "What is going on: `nn.Module` is a class defined by Pytorch developers. It implements the method `parameters()` which our model inherits from `nn.Module`. This method simply iterates over all parameterized function defined in the object's `__init__` method: here there is only an instance of `nn.Linear`. These functions are implemented by Pytorch in such a way that it knows how to list its own parameters, making all of this possible.\n",
    "\n",
    "Advanced note: When defining model parameters by hand, you may need to explicitely tell Pytorch that you did so with the method `register_parameter`. For now, don't worry about it since we will only be using functions provided by pytorch like `nn.Linear`, `nn.Conv1d`, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training objective\n",
    "In order to set our model parameters, we need an objective to minimize. Ideally, we would like to minimize the prediction error:\n",
    "$$\n",
    "    \\mathcal L^{pred}(\\tilde y, y) = \\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  0 \\text{ if } \\tilde y \\geq 0.5 \\text{ and } y=1\\\\\n",
    "                  0 \\text{ if } \\tilde y < 0.5 \\text{ and } y=0\\\\\n",
    "                  1 \\text{ otherwise,}\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "$$\n",
    "but it is not differentiable, so its optimization is untracktable. We will be optimizing the cross entropy loss, which is a widely used loss for classification tasks, and is defined by `torch.nn.functional.binary_cross_entropy` (`torch.nn.functional` was imported as `F`): https://pytorch.org/docs/stable/nn.functional.html?highlight=binary_cross#torch.nn.functional.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Train the model's parameters by minimizing the binary cross entropy loss. Plot the evolution of the loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run -i solutions/exo4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing $\\nabla_\\theta f$\n",
    "In general, $f_\\theta$ can be very complicated and include several milions of parameters: deriving its gradient by hand can be tidious if not physically impossible.\n",
    "\n",
    "However, $f_\\theta$ is often defined as a cascade of simple functions: linear functions $A_k: \\mathbb R^{d_k} \\mapsto \\mathbb R^{d_{k+1}}$ and non-linear activations $\\rho_k: \\mathbb R \\mapsto \\mathbb R$ applied to each coordinate independantly. The parameters here are the matrices $A_k$ which are regrouped into $\\theta = \\left( A_1 | \\dots | A_K \\right)$.\n",
    "\n",
    "This structure allows us to compute all gradients\n",
    "$$\n",
    "    \\nabla_\\theta f(x, \\theta) = \\left(\\ \\nabla_{A_1} f(x, \\theta) \\,| \\dots \\,|\\, \\nabla_{A_K} f(x, \\theta) \\ \\right)\n",
    "$$\n",
    "using the chain-rule. The resulting algorithm is called backpropagation, we will study it next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions in Pytorch\n",
    "\n",
    "Pytorch already knows many elementary functions such as sums, sine functions, norms, etc... Other functions can be obtained through the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Define the function $f(x, A, b) = \\max(0, Ax + b)$ where $x \\in \\mathbb R^{d_0}$, $A \\in \\mathcal M_{d_0, d_1}(\\mathbb R)$ and $b \\in \\mathbb R^{d_1}$.\n",
    "\n",
    "Note: $x \\mapsto \\max(0, x)$ is a widely used operation in deep learning, commonly refered to as ReLU for _Rectified Linear Unit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, A, b):\n",
    "    \"\"\"Compute the result of a fully connected layer y = max(0, Ax+b).\"\"\"\n",
    "\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0, d1 = 4, 3\n",
    "\n",
    "x = torch.randn(d0, requires_grad=True)\n",
    "A = torch.randn(d1, d0, requires_grad=True)\n",
    "b = torch.randn(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Compute $\\|f(x, A, b)\\|_2$ and its gradient with respect to $A$ and $b$ using the function `grad` as before. How could Pytorch know how to compute those gradients ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining new functions\n",
    "\n",
    "If the function we want to use can not be defined using basic Pytorch functions (typically because one step is unstable or has unstable gradient computation), we must define it ourselves.\n",
    "\n",
    "Pytorch's autograd package provides the class `Function` for this exact purpose:\n",
    "    https://pytorch.org/docs/stable/autograd.html?highlight=function#torch.autograd.Function\n",
    "\n",
    "Recall that in Pytorch, \"knowing\" a function a function means knowing how to compute it, as well as knowing how to compute its gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5: Define the inverse matrix function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseMatrix(Function):\n",
    "    \"\"\"A Function class for computing the inverse matrix, and its gradient.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, A):\n",
    "        \"\"\"Compute the inverse of matrix A\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"Compute the gradient of the loss with respect to A.\n",
    "        \n",
    "        grad_ouput contains the gradient of the loss with respect to A^-1\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
