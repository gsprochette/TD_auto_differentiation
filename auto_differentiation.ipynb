{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# auto differentiation library\n",
    "import torch\n",
    "import torch.nn as nn  # for defining Neural Networks\n",
    "import torch.nn.functional as F  # usual functions\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch's Tensors\n",
    "\n",
    "Pytorch contains a class `Tensor` which is a wrapper for array, matrices, tensors... But numpy is already able to handle usual linear algebra operation and coordinate-wise operations. Pytorch brings to the table __auto-differentiation__.\n",
    "That means a Tensor contains everything necessary to backpropagate gradients through the operation which created it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor type `x.type()` gives more insight on the data structure of tensor coordinates: torch.FloatTensor\n",
      "The tensor's shape, alternatively `x.size()` or `x.shape`: torch.Size([7])\n",
      "tensor([3., 1., 4., 1., 5., 9., 2.])\n",
      "You can get a numpy array back with `x.numpy()`: [3. 1. 4. 1. 5. 9. 2.]\n"
     ]
    }
   ],
   "source": [
    "# defining a Tensor from a numpy array\n",
    "x = np.array([3, 1, 4, 1, 5, 9, 2])\n",
    "x = torch.Tensor(x)\n",
    "print(\"Tensor type `x.type()` gives more insight on the data structure of tensor coordinates: {}\".format(x.type()))\n",
    "print(\"The tensor's shape, alternatively `x.size()` or `x.shape`: {}\".format(x.shape))\n",
    "print(x)\n",
    "print(\"You can get a numpy array back with `x.numpy()`: {}\".format(x.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`torch.mv` for matrix-vector multiplication: tensor([19.5277, -8.5126, -7.0295])\n",
      "Multiplication `*` is often reserved for coordinate wise multiplication.\n",
      "In Python, matrix multiplication is done using `@`: tensor([19.5277, -8.5126, -7.0295])\n"
     ]
    }
   ],
   "source": [
    "# linear algebra operation\n",
    "A = np.random.randn(3, x.shape[0])\n",
    "A = torch.Tensor(A)\n",
    "\n",
    "print(\"`torch.mv` for matrix-vector multiplication: {}\".format(torch.mv(A, x)))\n",
    "print(\"Multiplication `*` is often reserved for coordinate wise multiplication.\")\n",
    "print(\"In Python, matrix multiplication is done using `@`: {}\".format(A @ x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, this was the same as Numpy. Note that $x$ has a placeholder for a gradient, which will later contain $\\frac{\\partial \\mathcal L}{\\partial x}$ where $\\mathcal$ is any real value computed from $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch's Backpropagation\n",
    "Pytorch automatically registers operations when doing  operations. The `backward` method called on a scalar (torch Tensor of size 1) will backpropagate through the whole graph of dependancies. Calling `loss.backward` will compute for all variable used to compute `loss` the gradient of `loss`  with respect to that variable.\n",
    "\n",
    "For memory efficiency, intermediate gradients are deleted during the backpropagation, and unwanted gradients are never computed. If you explicitely want a gradient to be computed and kept, use the argument `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0568, -1.2728,  0.8023,  1.9957, -1.2047,  0.0181,  1.0877,  0.2828],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient at start: None\n",
      "L1 norm: 6.7208251953125\n",
      "Gradient after `l1.backward()`: tensor([-1., -1.,  1.,  1., -1.,  1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, requires_grad=True)\n",
    "display(x)\n",
    "\n",
    "print(\"Gradient at start: {}\".format(x.grad))\n",
    "\n",
    "l1 = torch.sum(torch.abs(x))\n",
    "print(\"L1 norm: {}\".format(l1))\n",
    "\n",
    "l1.backward()\n",
    "print(\"Gradient after `l1.backward()`: {}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that gradients accumulate, and if you want to compute a new gradient from the same vector you need to delete its gradient first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without clean up:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 2., 2., 0., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "x.grad = torch.ones_like(x)  # artificial gradient from previous computation\n",
    "\n",
    "print(\"Without clean up:\")\n",
    "print(x.grad)\n",
    "\n",
    "l1 = torch.sum(torch.abs(x))\n",
    "l1.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With clean up:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([-1., -1.,  1.,  1., -1.,  1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "x.grad = torch.ones_like(x)  # artificial gradient from previous computation\n",
    "\n",
    "print(\"With clean up:\")\n",
    "print(x.grad)\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "l1 = torch.sum(torch.abs(x))\n",
    "l1.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only tensors with `requires_grad=True` keep their gradients.\n",
      "tensor([ 0.0432,  0.0064, -0.0102, -0.0361, -0.0344,  0.0391, -0.0567, -0.0050])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, requires_grad=True)\n",
    "\n",
    "y = torch.abs(x)  # modulus coordinate wise\n",
    "z = x ** 2  # square coordinate wise\n",
    "\n",
    "l1 = torch.sum(y)\n",
    "l2 = torch.sqrt(torch.sum(z))\n",
    "\n",
    "s = l2 / l1  # some real value depending on x. It turns out this is a good measure for sparsity\n",
    "\n",
    "# Backpropagate gradients:\n",
    "s.backward()\n",
    "\n",
    "print(\"Only tensors with `requires_grad=True` keep their gradients.\")\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative way to get gradients is to use the function `grad` provided in the `autograd` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 1., -1., -1., -1.,  1.,  1., -1., -1.]),)\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "x = torch.randn(8, requires_grad=True)\n",
    "l1 = torch.sum(torch.abs(x))\n",
    "\n",
    "# grad's input is a list of losses and a list of tensors with respect to which we want the gradient.\n",
    "gx = grad([l1], [x])\n",
    "print(gx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch's Modules\n",
    "\n",
    "You can functions to manipulate tensors. However, the final goal is to define a __parameterized__ function. In Pytorch, this is the purpose of `nn.Module`, from which modules should inherit.\n",
    "\n",
    "A new module should define its initialization process (in particular it must initialize its parameters), and the forward function. The backward function is automatically defined and needs not be worried about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the architecture of the network\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        \"\"\"\n",
    "        A shallow network which performs logistic regression.\n",
    "        \n",
    "        Arguments:\n",
    "        ----------\n",
    "        p: dimension of the input\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()  # run the initialiazation code of inherited class nn.Module\n",
    "        \n",
    "        self.linear = nn.Linear(p, 1)  # parameterized function defined by pytorch, x -> Ax + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: x must be of size p\n",
    "        xw = self.linear(x)\n",
    "        y = torch.sigmoid(xw)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an instance of the nework with randomly chosen parameters\n",
    "p = 8\n",
    "network = LogisticRegression(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5915]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Calling the forward function can be done either by treating the network as a function,\n",
    "# or by explicitely calling the forward method. The first option is cleaner.\n",
    "x = torch.randn(1, p)\n",
    "\n",
    "y_pred = network(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a loss is done outside the network\n",
    "\n",
    "# artificial label. x.detach() is used to detach this tensor from previously defined tensors.\n",
    "y_true = .5 * (1 + torch.sign(x.detach()[:, 0:1]))\n",
    "\n",
    "mse_loss = F.binary_cross_entropy(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling backward\n",
    "mse_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could do the update by hand, but we will want to use other optimization schemes. Pytorch provides optimizers, which implement the descent and can be used after `.backward()` with a simple call of the method `.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "optimizer = optim.SGD(network.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer also have a very nice way to reset gradients, which also affects the descent algorithm if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set gradients to zero\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batches\n",
    "You may have noticed that we have added a useless dimension in $x$, this is for the batch size. In practice, performing gradient steps on the whole training set is too costly, and doing it on a single datapoint is too unstable.\n",
    "As a tradeoff, we use a subset of data at each step, not too many and not too few. These subsets are called batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tensor dimension\n",
    "Pytorch has a convention for the dimension of network inputs:\n",
    "- first the batch dimension: x[i, ...] is the i-th sample in the batch.\n",
    "- second, the channels: x[i, c, ...] is the c-th channel. For example, images may have 3 channels: red, green and blue.\n",
    "- last, 0, 1, 2 our 3 spatial dimension depending on the data: x[..., x, y, z] is the voxel (x, y, z) of a 3D volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: with a loop, perform gradient steps and train the network.\n",
    "Here, unlimited data is available with `get_data_point(batch_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_point(batch_size):\n",
    "    x = torch.randn(batch_size, p)\n",
    "    y = .5 * (1 + torch.sign(x.detach()[:, 0:1]))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
